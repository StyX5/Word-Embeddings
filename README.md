# Word-Embeddings
 Skip-gram with negative sampling

In this lab, we will learn about one of the two word2vec variants: skip-gram with negative sampling. We will derive and implement the model by hand using NumPy, and train it on a subset of the Internet Movie Database (IMDB) dataset. Readings: the original paper introducing skip-gram (along with the other word2vec variant, CBOW) is [Mikolov et al., 2013a]. Negative sampling was proposed in a follow-up paper [Mikolov et al., 2013b]. Helpful resources to understand how skip-gram works are [Goldberg et Levy, 2014] and subsection 3.1 of [Bojanowski et al., 2017].
